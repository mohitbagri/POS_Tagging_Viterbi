{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS_530_Homework_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGbehYhTiA9f"
      },
      "source": [
        "POS Tagging using Viterbi Algorithm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytfBTZK1aB_2"
      },
      "source": [
        "Run the cell below to import everything needed to complete the rest of this notebook. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFfbuQep3eUb"
      },
      "source": [
        "import nltk\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "jA2NaMRkLdS-",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "017ab02f3a93b9f463878f88c4ff6797",
          "grade": false,
          "grade_id": "cell-70a6adf192c9ef57",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## Section 1: POS Tagging with HMM\n",
        "\n",
        "A Hidden Markov Model is a probabilistic sequence model that, given a sequence of words, computes a probability distribution over a sequence of POS tags. Given a trained HMM, we want to compute the likelihood of a particular sequence using a forward algorithm.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxfbulJVUUwz"
      },
      "source": [
        "### 1.1 Exploring the Dataset\n",
        "\n",
        "We will be using the tagged sentences in the Brown corpus. Let's first take a look at the data we are working with.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBCqp7ckKobI"
      },
      "source": [
        "corpus = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cb0cpiTMFuE"
      },
      "source": [
        "# first two tagged sentences in the corpus\n",
        "pp = pprint.PrettyPrinter()\n",
        "pp.pprint(corpus[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGOHpIN2ucDq"
      },
      "source": [
        "What are the unique tags that are in this data? Save this into a variable `tags`.\n",
        "\n",
        "Also, what are the unique words that are in this data? Save this into a variable `words`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI6kz7wqQgs-"
      },
      "source": [
        "new_corpus=[]\n",
        "for i in corpus:\n",
        "    i.insert(0, ('<s>','START'))\n",
        "    x = ('</s>','END')\n",
        "    i.append(x)\n",
        "    new_corpus.append(i)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N1hyEclV6rJ"
      },
      "source": [
        "tags_list=[]\n",
        "words_list=[]\n",
        "for index1,sentence in enumerate(new_corpus):\n",
        "  for index2, pair in enumerate(sentence):\n",
        "    words_list.append(pair[0])\n",
        "    tags_list.append(pair[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_AxQNQ0YDh6"
      },
      "source": [
        "tags = set(tags_list)\n",
        "words = set(words_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cihWx-RLUaBg"
      },
      "source": [
        "### 1.2 Evaluating Likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wya28IhhyM7W"
      },
      "source": [
        "Now that we've taken a look at the dataset, let's use the Hidden Markov Model to calculate the emission probabilities and transition probabilities. \n",
        "\n",
        "We'll start with the emission probabilities. Recall that the emission probability computes the probability of a word given a tag. In other words, the emission probability can be calculated by dividing the number of times a word is given a tag by the total count of that tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9E7DJvwXh7R"
      },
      "source": [
        "To calculate the accuracy of the Viterbi algorithm that we will implement later, let's divide the corpus up into a training and a test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IURl9PfZvQF"
      },
      "source": [
        "# splitting into train and test sets\n",
        "train_set, test_set = train_test_split(new_corpus,train_size = 0.95, test_size = 0.05, random_state = 123)\n",
        "\n",
        "# putting those into their own lists\n",
        "train_words = [tup for sent in train_set for tup in sent]\n",
        "test_words = [tup[0] for sent in test_set for tup in sent]\n",
        "\n",
        "# print out first five train set words\n",
        "pp.pprint(train_words[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVuAPuQ9wed_"
      },
      "source": [
        "word_list = []\n",
        "\n",
        "for i in train_words:\n",
        "  word_list.append(i[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iE9UODNYIrW"
      },
      "source": [
        "Now let's calculate the emission and transition probabilities. We will use this to assign a tag `t` \n",
        "to each word `w` which maximizes the likelihood of `P(t|w)`. Recall from lecture that by using Bayes rule, this can be computed with `P(w|t)` (emission probability) and `P(t|t-1)` (transition probability)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjT6zIqCxxDM"
      },
      "source": [
        "Create a matrix `emissions` which, for each word as the key, stores the key-value pair of POS tags and the count of that word under that POS tag. Note that this does not calculate the probability, but will be used as a helper function in our Viterbi algorithm. Be sure to use the words from the training set `train_words` to create this matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHRruZTqY2RD"
      },
      "source": [
        "pos_map={}\n",
        "for index1,pair in enumerate(train_words):\n",
        "    key = pair[1] + \"*\" + pair[0]\n",
        "    if pos_map.get(key) is None:\n",
        "      pos_map[key] = 1\n",
        "    else:\n",
        "      pos_map[key] = pos_map[key] + 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P03Lcd8JD-Ap"
      },
      "source": [
        "print(pos_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gIjOJXtGGvo"
      },
      "source": [
        "emissions = {}\n",
        "for item in pos_map.items():\n",
        "  pos_word_pair = item[0].split(\"*\",1)\n",
        "  pos_word_pair.reverse()\n",
        "  count = str(item[1])\n",
        "  temp={}\n",
        "  temp[pos_word_pair[-1]] = int(count)\n",
        "  if emissions.get(pos_word_pair[0],'empty')=='empty':\n",
        "    emissions[pos_word_pair[0]]={}\n",
        "  emissions[pos_word_pair[0]].update(temp)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHu_wSVcyeGu"
      },
      "source": [
        "print(emissions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0MwjgpONuft"
      },
      "source": [
        "Now, we will create the emission matrix using the emissions dictionary we created, and save it to a dataframe. This may be used in the implementation of the Viterbi algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swyIQmGY3mwf"
      },
      "source": [
        "emissions_df = pd.DataFrame(emissions).transpose()\n",
        "emissions_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aGBk0vQyuSC"
      },
      "source": [
        "Likewise, implement `transitions` which stores as keys the POS tags, and stores as values the key (previous POS tag) - value (number of times the previous POS tag is followed by the current POS tag) pair. We include the `START` and `END` tags. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWBS5KiDeL3g"
      },
      "source": [
        "trans_map={}\n",
        "print(len(train_words))\n",
        "i=0\n",
        "for index1,pair in enumerate(train_words):\n",
        "    if index1==0 : \n",
        "      continue   \n",
        "    prev_pair = train_words[index1-1]\n",
        "    pos_key = pair[1] +  '*' + prev_pair[1]\n",
        "    if pos_key in trans_map:\n",
        "      trans_map[pos_key] = trans_map[pos_key] + 1\n",
        "    else:\n",
        "      trans_map[pos_key] = 1    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdUI4eYUDDSs"
      },
      "source": [
        "print(trans_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "210LW4mt10Y5"
      },
      "source": [
        "transitions = {}\n",
        "for item in trans_map.items():\n",
        "  pos_word_pair = item[0].split(\"*\")\n",
        "  count = str(item[1])\n",
        "  temp={}\n",
        "  temp[pos_word_pair[1]] = int(count)\n",
        "  if transitions.get(pos_word_pair[0],'empty')=='empty':\n",
        "    transitions[pos_word_pair[0]]={}\n",
        "  transitions[pos_word_pair[0]].update(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMPcQsxaKGW7"
      },
      "source": [
        "Again, we will create the transition matrix (`t` x `t`) using the transition probability dictionary we created, and save it to a dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHv9h5uZAJ6q"
      },
      "source": [
        "transitions_df = pd.DataFrame(transitions).T\n",
        "transitions[\"START\"][\"END\"] = 0\n",
        "display(transitions_df)\n",
        "#transitions_df['ADJ']['NOUN']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "qic4NvXX3eUa",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "449bd7b2b6cfbf01f5b06bbdd40192a9",
          "grade": false,
          "grade_id": "cell-3525243964a48b38",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 1.3: Decoding The Input - Viterbi Algorithm \n",
        "\n",
        "The Viterbi Algorithm computes the **most likely sequence** of hidden states, given the evidence that is fed into a HMM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32JR8rhkZl7m"
      },
      "source": [
        "Below is a function `localMaximizer' that only considers forward probabilities and thus leads to only a local optimum, not to a global optimum. This is **NOT** what the Viterbi algorithm does -- we would instead like to store a global optimum using (1) a partial, best forward probability, and (2) a backpointer to what the best predecessor is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY3P4iNFZnhb"
      },
      "source": [
        "\n",
        "def localMaximizer(words, train_on = train_words):\n",
        "  tags = list(set([tup[1] for tup in train_on])) \n",
        "  states = []\n",
        "\n",
        "  # go through the sequence of words\n",
        "  for key, word in enumerate(words):\n",
        "    # initialize matrices\n",
        "    prob = []\n",
        "    for tag in tags:\n",
        "      if key == 0:\n",
        "        transition = transitions_df.loc['END', tag]\n",
        "      else:\n",
        "        transition = transitions_df.loc[states[-1], tag]\n",
        "        \n",
        "      if emissions.get(word, {}).get(tag):\n",
        "        emission = emissions[words[key]][tag]\n",
        "      else:\n",
        "        emission = 0.0\n",
        "\n",
        "      # calculate emission probabilities * transition probabilities for all tags\n",
        "      prob.append(transition * emission)\n",
        "\n",
        "    # find the maximum probability state, backtrace\n",
        "    max_state = tags[prob.index(max(prob))]\n",
        "    states.append(max_state)\n",
        "  \n",
        "  return list(zip(words, states))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Dq2gqIZq2w"
      },
      "source": [
        "Let's check out how this local maximizer does on our test set. Below, we create a list of test sentences with words that have tags, and another list of those same words but without tags. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_nPk8XeZphR"
      },
      "source": [
        "tests = test_set\n",
        "\n",
        "# words with tags\n",
        "test_with_tags = [tup for sent in tests for tup in sent]\n",
        "# words without their tags\n",
        "test_without_tags = [tup[0] for sent in tests for tup in sent]\n",
        "\n",
        "find_sequence = localMaximizer(test_without_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjztqJzXaYjl"
      },
      "source": [
        "And below, we calculate the accuracy that was achieved based on this test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd1S_lAgZvot"
      },
      "source": [
        "correct = []\n",
        "for i, j in zip(find_sequence, test_with_tags):\n",
        "  if i == j:\n",
        "    correct.append(i)\n",
        "\n",
        "accuracy = len(correct)/len(find_sequence)\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKatTJ6gZvWm"
      },
      "source": [
        "Let's improve this now by implementing the Viterbi algorithm properly. \n",
        "\n",
        "We implement the [Viterbi Algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm#Pseudocode) using the pseudocode at the link. It will be helpful to use the helper functions above and both the `emissions_df` and `transitions_df` in our implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZjsgnE78LVD"
      },
      "source": [
        "transitions_df=transitions_df.T\n",
        "index_list=transitions_df.index.values.tolist()\n",
        "temp1_df=transitions_df[index_list]\n",
        "col_list=list(transitions_df.columns) \n",
        "temp2_df=emissions_df.T.reindex(index_list)\n",
        "temp3_df=emissions_df.T.reindex(col_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u8kI06FV0MT"
      },
      "source": [
        "class Viterbi():\n",
        "  def __init__(self, Y):\n",
        "    self.O = list(set([w_t[0] for w_t in train_words]))  # Observation space  \n",
        "    self.S=  col_list  # State space\n",
        "    self.Y = Y  # Sequence of observations\n",
        "    self.A = np.log(transitions_df.T+1)  # Transition matrix\n",
        "    #self.A= temp1_df\n",
        "    self.B = np.log(temp3_df+1)   # Emission matrix\n",
        "    self.N = len(self.O)\n",
        "    self.K = len(self.S)\n",
        "    self.T = len(Y)\n",
        "    self.T1 = [[0] * self.T for i in range(self.K)]\n",
        "    self.T2 = [[None] * self.T for i in range(self.K)]\n",
        "    self.X = [None] * self.T # Predicted tags\n",
        "\n",
        "    self.training_word_list=[]\n",
        "\n",
        "  def decode(self):\n",
        "\n",
        "    for word in train_words:\n",
        "      self.training_word_list.append(word[0])\n",
        "    \n",
        "    for i in range(0,self.K):\n",
        "      if self.Y[0] in self.training_word_list:\n",
        "        self.T1[i][0] = self.A.iloc[0][i] + self. B.iloc[i][self.Y[0]]\n",
        "      else:\n",
        "          self.T1[i][0] = self.A.iloc[0][i] + self. B.iloc[i][self.Y[1]]\n",
        "      self.T2[i][0] = 0\n",
        "\n",
        "    for j in range(1, self.T):\n",
        "      argmax_k=1\n",
        "      temp_list=[]\n",
        "      for i in range(0,self.K):\n",
        "        for k in range(0,self.K):\n",
        "          if self.Y[j] not in self.training_word_list:\n",
        "            self.T1[i][j] = max(self.T1[i][j],100)\n",
        "          else:  \n",
        "            pos = self.S[k]   \n",
        "            if self.T1[i][j] < self.T1[k][j-1] + self.A[pos][i] + self.B.iloc[i][self.Y[j]]:      \n",
        "              self.T1[i][j] = max(self.T1[i][j],self.T1[k][j-1] + self.A[pos][i] + self.B.iloc[i][self.Y[j]])\n",
        "              pair=(self.T1[k][j-1] + self.A[pos][i] + self.B.iloc[i][self.Y[j]],k)\n",
        "              temp_list.append(pair)\n",
        "              argmax_k=k\n",
        "        self.T2[i][j] = argmax_k\n",
        "    z= [0] * self.T\n",
        "    max_val=0\n",
        "    for k in range(0,self.K):\n",
        "      if max_val < self.T1[k][self.T-1]:\n",
        "        max_val = self.T1[k][self.T-1]\n",
        "        z[self.T-1] = k  \n",
        "    self.X[self.T-1] = self.S[z[self.T-1]]\n",
        "    for j in range(self.T-1, 0, -1):\n",
        "        if self.T2[z[j]][j] is not None and self.Y[j]  in self.training_word_list:\n",
        "          z[j-1] = self.T2[z[j]][j]\n",
        "          self.X[j-1] = self.S[z[j-1]]\n",
        "        else:          \n",
        "          temp_list=np.array([row[j-1] for row in self.T1])\n",
        "          tag = self.S[np.argmax(temp_list)]\n",
        "          self.X[j-1]=tag    \n",
        "    return self.X    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUGPGxkxnn4Y"
      },
      "source": [
        "**Since the Viterbi Algorithm takes a long time to run, we split the test set into batches of 1000 words and we are currently running only on the first 5000 words of the test set.**\n",
        "\n",
        "**Each batch of 500 words takes 2 minutes to run and the output of each batch is printed**\n",
        "\n",
        "**To run on the entire test please change the variable named \"TEST_DATA_SIZE\" from 2000 to [number of words in test data]**\n",
        "\n",
        "**This cell will take 10 minutes to run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf3AA1TTh8HS"
      },
      "source": [
        "tests = test_set\n",
        "\n",
        "TEST_DATA_SIZE=2000\n",
        "BATCH_SIZE=500\n",
        "mislabels=[]\n",
        "\n",
        "count=1\n",
        "for i in range(0,TEST_DATA_SIZE,BATCH_SIZE):\n",
        "  test_with_tags = [tup for sent in tests for tup in sent][i:i+BATCH_SIZE]\n",
        "  test_without_tags = [tup[0] for sent in tests for tup in sent][i:i+BATCH_SIZE]\n",
        "  print(\"EXECUTING BATCH: \", count,\" from word index \", i, \"till word index \", (i+BATCH_SIZE))\n",
        "  decoder = Viterbi(test_without_tags)\n",
        "  pred = decoder.decode()\n",
        "  result = [(test_without_tags[i], pred[i]) for i in range(0, len(pred))] \n",
        "  print(\"VITERBI RESULT :\", result)\n",
        "  print(\"ACTUAL TAGS: \" , test_with_tags)\n",
        "  correct = []\n",
        "  for i, j in zip(result, test_with_tags):\n",
        "    if i == j:\n",
        "      correct.append(i) \n",
        "    else:\n",
        "      error=(i,j)\n",
        "      mislabels.append(error)  \n",
        "  accuracy = len(correct)/len(result)\n",
        "  print(\"ACCURACY: \" , accuracy)\n",
        "  count =  count+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar3wDCgLUf7h"
      },
      "source": [
        "Let's see how the Viterbi implementation performs on sentences in our test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifg-zBGUVsCZ"
      },
      "source": [
        "And below, we calculate the accuracy that was achieved based on this test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaUsUfy7W3qR"
      },
      "source": [
        "What are some of the words that were tagged incorrectly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqNLLmXkXzlM"
      },
      "source": [
        "for index, pair in enumerate(mislabels):\n",
        "  print(\"Predicted word-tag : \", pair[0], \" Actual word-tag : \", pair[1])\n",
        "  if index > 15:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}